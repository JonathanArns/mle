% Encoding: UTF-8

@inproceedings{the_annotated_transformer,
    title = "The Annotated Transformer",
    author = "Rush, Alexander",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2509",
    doi = "10.18653/v1/W18-2509",
    pages = "52--60",
    abstract = "A major goal of open-source NLP is to quickly and accurately reproduce the results of new work, in a manner that the community can easily use and modify. While most papers publish enough detail for replication, it still may be difficult to achieve good results in practice. This paper presents a worked exercise of paper reproduction with the goal of implementing the results of the recent Transformer model. The replication exercise aims at simple code structure that follows closely with the original work, while achieving an efficient usable system.",
}

@inproceedings{attention_is_all_you_need,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, undefinedukasz and Polosukhin, Illia},
    title = {Attention is All You Need},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {6000–6010},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}

@ARTICLE{comparative_study_cnn_rnn,
    author = {{Yin}, Wenpeng and {Kann}, Katharina and {Yu}, Mo and {Sch{\"u}tze}, Hinrich},
    title = "{Comparative Study of CNN and RNN for Natural Language Processing}",
    journal = {arXiv e-prints},
    keywords = {Computer Science - Computation and Language},
    year = 2017,
    archivePrefix = {arXiv},
    eprint = {1702.01923},
    adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170201923Y},
}

@inproceedings{sequence_to_sequence,
    author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
    title = {Sequence to Sequence Learning with Neural Networks},
    year = {2014},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
    booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
    pages = {3104–3112},
    numpages = {9},
    location = {Montreal, Canada},
    series = {NIPS'14}
}

@inproceedings{visualization_of_attention,
    title = "A Multiscale Visualization of Attention in the Transformer Model",
    author = "Vig, Jesse",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-3007",
    doi = "10.18653/v1/P19-3007",
    pages = "37--42",
    abstract = "The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.",
}

@article{softmax_to_softassign,
    title={Softmax to softassign: Neural network algorithms for combinatorial optimization},
    author={Gold, Steven and Rangarajan, Anand and others},
    journal={Journal of Artificial Neural Networks},
    volume={2},
    number={4},
    pages={381--399},
    year={1996},
    publisher={Ablex Publishing Corp. Norwood, NJ, USA}
}

@article{activation_functions,
    title={Searching for activation functions},
    author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
    journal={arXiv preprint arXiv:1710.05941},
    year={2017}
}

@article{gpt_1,
    title={Improving language understanding by generative pre-training},
    author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
    year={2018},
    journal={OpenAI},
    publisher={OpenAI}
}

@article{gpt_2,
    title={Language Models are Unsupervised Multitask Learners},
    author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year={2019},
    journal={OpenAI},
    publisher={OpenAI}
}

@inproceedings{beam_search,
    author = {Furcy, David and Koenig, Sven},
    title = {Limited Discrepancy Beam Search},
    year = {2005},
    publisher = {Morgan Kaufmann Publishers Inc.},
    address = {San Francisco, CA, USA},
    abstract = {Beam search reduces the memory consumption of best-first search at the cost of finding longer paths but its memory consumption can still exceed the given memory capacity quickly. We therefore develop BULB (Beam search Using Limited discrepancy Backtracking), a complete memory-bounded search method that is able to solve more problem instances of large search problems than beam search and does so with a reasonable runtime. At the same time, BULB tends to find shorter paths than beam search because it is able to use larger beam widths without running out of memory. We demonstrate these properties of BULB experimentally for three standard benchmark domains.},
    booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
    pages = {125–131},
    numpages = {7},
    location = {Edinburgh, Scotland},
    series = {IJCAI'05}
}

@article{bert,
    title={BERT rediscovers the classical NLP pipeline},
    author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
    journal={arXiv preprint arXiv:1905.05950},
    year={2019}
}

@article{hello_its_gpt,
    title={Hello, it's GPT-2--how can I help you? towards the use of pretrained language models for task-oriented dialogue systems},
    author={Budzianowski, Pawe{\l} and Vuli{\'c}, Ivan},
    journal={arXiv preprint arXiv:1907.05774},
    year={2019}
}

@misc{illustrated_transformer,
    title={The Illustrated Transformer},
    author={Allamar, Jay},
    year={2018},
    note={http://jalammar.github.io/illustrated-transformer/ (aufgerufen am 26.02.2021)}
}

@article{pretrained_models,
    title={Pre-trained models for natural language processing: A survey},
    author={Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
    journal={Science China Technological Sciences},
    pages={1--26},
    year={2020},
    publisher={Springer}
}